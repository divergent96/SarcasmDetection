{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# Set random seed\n\nSEED = 1234509876\n\n# Importing basic libraries\nfrom zipfile import ZipFile\nimport os, sys\nimport re\nimport gc\nimport time\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json \nfrom string import punctuation\nimport pickle as pkl\n\n%matplotlib inline\n\n# Import NLTK\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n# Preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import f1_score\n\nfrom wordcloud import WordCloud\n# Import models\n\nimport lightgbm as lgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# Model selection\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Others\n\nfrom tqdm import tqdm_notebook #Loads progressbars for various loops\n\nfrom typing import List\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n#####################\n#Useful pandas settings\n\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 40)\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:02.276499Z","iopub.execute_input":"2023-02-14T18:02:02.277000Z","iopub.status.idle":"2023-02-14T18:02:06.087485Z","shell.execute_reply.started":"2023-02-14T18:02:02.276904Z","shell.execute_reply":"2023-02-14T18:02:06.086290Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Custom Functions\n\nCollecting all functions here for easy reference and update","metadata":{}},{"cell_type":"code","source":"################################################################################################\n\n# Downcasting function for pandas dataframes\n\ndef downcast_dtypes(df):\n  '''\n      Changes column types in the dataframe:             \n          `float64` type to lowest possible float without data loss\n          `int64`   type to lowest possible int wihtout data loss\n  '''\n  \n  # Select columns to downcast\n  float_cols = [col for col in df if df[col].dtype == \"float64\"]\n  int_cols =   [col for col in df if df[col].dtype == \"int64\"]\n  \n  # Downcast columns using to numeric function\n  df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n  df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n\n  # remove variables from memory to avoid issues\n  \n  del float_cols\n  del int_cols\n  \n  return df\n\n################################################################################################\n# Check duplication at given level of dataframe\n\ndef check_dups(df, cols):\n\n  orig_count_rows = df.shape[0]\n\n  temp = df.groupby(cols).size().reset_index(name = 'counts')\n\n  dedup_count_rows = temp.shape[0]\n\n  if orig_count_rows == dedup_count_rows:\n    print(\"No duplicates. Dataframe is unique at given level\")\n    print(\"# of unique entries: n=\",orig_count_rows)\n  else:\n    print(\"Duplicates found. Dataframe is not unique at given level\")\n    print(\"# of entries in original dataset: n=\", orig_count_rows)\n    print(\"# of unique entries expected in deduped dataset: n=\", dedup_count_rows)\n    print(\"# of addational entries: n=\", orig_count_rows - dedup_count_rows)\n\n  del orig_count_rows, temp, dedup_count_rows\n\n#####################################################################################\n# Plotting classification features\ndef fancy_plot(df):\n  column_names = list(df.columns.values)\n  frauds = df[df['Class'] == 1]\n  no_frauds = df[df['Class'] == 0]\n\n  plt.figure()\n  fig, ax = plt.subplots(8,4,figsize=(16,28))\n  i = 0\n  for feature in column_names:\n      i += 1\n      plt.subplot(8,4,i)\n      sns.kdeplot(frauds[feature])\n      sns.kdeplot(no_frauds[feature])\n      plt.xlabel(feature, fontsize=10)\n      locs, labels = plt.xticks()\n      plt.tick_params(axis='both', which='major', labelsize=12)\n  plt.show();\n\n####################################################################################\n\n########################################\n#Custom function to apply functions to dataframe with missing values\n\ndef impute_missing(df, func, target_col, new_col_name):\n  df.loc[~df[target_col].isnull(),new_col_name] = df.loc[~df[target_col].isnull(),target_col].apply(func)\n\n\n####################################################################################\n#text cleaning and stemming function. Modified to cater to text provided\n\ndef remove_links(raw):\n    # Extracts links from input text. Returns both text and links \n    link_expr = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+.'\n    \n    # Check if passed object is single string or series\n    if type(raw) == str:\n        no_link_raw = re.sub(link_expr,\"\",raw)\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        no_link_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            no_link_raw.append(re.sub(link_expr, \"\", tweet))\n            \n    return no_link_raw\n    \ndef remove_hashtags(raw):\n    # Extracts links from input text. Returns both text and links\n    # Will remove all trailing hashtags\n    # Hashtags in middle of text will be replaced by a \"SOME_ENTITY\" constant text with hoope to retain context\n    \n    hashtag_expr = '#[A-Za-z0-9]+'\n    middle_tag_expr = '#[A-Za-z0-9]+^[#]'\n    # tags = re.findall(hashtag_expr,\"\",raw)\n    \n    if type(raw) == str:\n        no_tag_raw = re.sub(hashtag_expr,\"\",raw)\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        no_tag_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            no_tag_raw.append(re.sub(hashtag_expr, \"\", tweet))\n            \n    return no_tag_raw\n    \ndef replace_mentions(raw):\n    # Replaces personal mentions with a common entity tag.\n    # As we cannot build context on specific persons, we will tag it as entity and let our model identify language patterns\n    mention_expr = '@[A-Za-z0-9]+'\n    # tags = re.findall(hashtag_expr,\"\",raw)\n    \n    if type(raw) == str:\n        no_mention_raw = re.sub(mention_expr,\"SOME_ENTITY\",raw)\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        no_tag_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            no_tag_raw.append(re.sub(mention_expr,\"SOME_ENTITY\",tweet))\n            \n    return no_tag_raw\n\ndef trim_extra_space(raw):\n    space_expr = '\\s+'\n    # tags = re.findall(hashtag_expr,\"\",raw)\n    \n    if type(raw) == str:\n        clean_raw = re.sub(space_expr,\" \",raw)\n        clean_raw = clean_raw.strip(\" \") # Remove end trails\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        clean_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            temp = re.sub(space_expr,\" \",tweet)\n            clean_raw.append(temp.strip(\" \"))\n            \n    return clean_raw\n\ndef clean_text(raw):\n    # Combine all cleaning work\n    cleaned_text = remove_links(raw)\n    cleaned_text = remove_hashtags(cleaned_text)\n    cleaned_text = replace_mentions(cleaned_text)\n    cleaned_text = trim_extra_space(cleaned_text)    \n\n    return cleaned_text\n\n    \n# def token_converter():\n    # Convert text to tokens\n    \n#     tokens = nltk.word_tokenize(temp)\n    \n#     alph_num_tokens = [word for word in tokens if word.isalnum()]\n#     non_alph_num_tokens = [word for word in tokens if not word.isalnum()]\n\n#     non_alph_num_tokens = [word.split('-') for word in non_alph_num_tokens]\n#     non_alph_num_tokens = nltk.flatten(non_alph_num_tokens)\n#     non_alph_num_tokens = [word.split('.') for word in non_alph_num_tokens]\n#     non_alph_num_tokens = nltk.flatten(non_alph_num_tokens)\n\n#     alph_num_tokens.extend(non_alph_num_tokens)\n\n#     tokens = nltk.flatten(alph_num_tokens)\n\n#     tokens = [porter.stem(word.lower()) for word in tokens]\n#     tokens = [word for word in tokens if word not in stopwords_en]\n#     tokens = [word for word in tokens if word.isalnum()]\n\n#     return tokens\n\n    #####################################################\n# Generate word clouds\n\ndef generate_wordclouds(X, in_X_tfidf, k, in_word_positions):\n    # compute the total tfidf for each term in the cluster\n    in_tfidf = in_X_tfidf[in_y_pred == in_cluster_id]\n    # numpy.matrix\n    tfidf_sum = np.sum(in_tfidf, axis=0)\n    # numpy.array of shape (1, X.shape[1])\n    tfidf_sum = np.asarray(tfidf_sum).reshape(-1)\n    top_indices = tfidf_sum.argsort()[-top_count:]\n    term_weights = {in_word_positions[in_idx]: tfidf_sum[in_idx] for in_idx in top_indices}\n    wc = WordCloud(width=1200, height=800, background_color=\"white\")\n    wordcloud = wc.generate_from_frequencies(term_weights)\n    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    fig.suptitle(f\"Cluster {in_cluster_id}\")\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:06.091917Z","iopub.execute_input":"2023-02-14T18:02:06.092303Z","iopub.status.idle":"2023-02-14T18:02:06.123234Z","shell.execute_reply.started":"2023-02-14T18:02:06.092268Z","shell.execute_reply":"2023-02-14T18:02:06.122274Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Import dataset\n\nDataset has been uploaded to kaggle repo for easy access","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/sarcasm/all_twitter_sarcasam.csv')\n\n# Remove extra columns from the data\nkeep_cols = ['id','text']\nraw_data = raw_data.loc[:,keep_cols]\n","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:06.124478Z","iopub.execute_input":"2023-02-14T18:02:06.125294Z","iopub.status.idle":"2023-02-14T18:02:06.660136Z","shell.execute_reply.started":"2023-02-14T18:02:06.125238Z","shell.execute_reply":"2023-02-14T18:02:06.658856Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleanup\n\nMay need to label data before doing cleanup\n\nFigure out a way to tag hashtags in middle of text (maybe after cleanup since they","metadata":{}},{"cell_type":"code","source":"raw_data['clean_text'] = clean_text(raw_data['text'])\n\n# Remove duplicates. There are ton of ads that will be easy to remove after cleanup\n# Ads contain different hashtags so cant be deduped raw\n\nraw_data.drop_duplicates(subset = 'clean_text', inplace = True)\nraw_data = raw_data.reset_index().drop(columns = 'index')","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:06.662722Z","iopub.execute_input":"2023-02-14T18:02:06.663183Z","iopub.status.idle":"2023-02-14T18:02:06.738350Z","shell.execute_reply.started":"2023-02-14T18:02:06.663145Z","shell.execute_reply":"2023-02-14T18:02:06.737212Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"raw_data[1:20]","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:06.739949Z","iopub.execute_input":"2023-02-14T18:02:06.740293Z","iopub.status.idle":"2023-02-14T18:02:06.759643Z","shell.execute_reply.started":"2023-02-14T18:02:06.740263Z","shell.execute_reply":"2023-02-14T18:02:06.757898Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                     id                                     text  \\\n1   1623470696125923329  Get my art printed on awesome produc...   \n2   1623467236982947842  Trudeau? anyone? #TuckerCarlson #Unh...   \n3   1623465163792711681  Nuh uh. #joebiden told me everything...   \n4   1623465117869395968  😂 she gave #Biden the #ChinaBalloon ...   \n5   1623464838990295040  He’s afraid of me because of my eye ...   \n6   1623463889194344449  How old do I have to be before I hav...   \n7   1623463133170868224  https://t.co/DOHN1RsqCg… Very funny ...   \n8   1623461020382494720  Hmm ...  Can't attach pics, now, too...   \n9   1623460933900066816  Looks like this toerag has really an...   \n10  1623434796432248833  I had a good laugh today responding ...   \n11  1623433543203405827  I wonder how many crypto commercials...   \n12  1623430337786445824  @RepDonaldsPress So it's still on th...   \n13  1623430319348264960  Real Cats Eat Hotdog\\nhttps://t.co/Z...   \n14  1623429168745570309  Some need to be informed this is\\n#s...   \n15  1623423936812748800  @DocumentingBTC \\n\\nIt's so nice to ...   \n16  1623420440717799427  Funny Sassy Kitchen Towel - Embroide...   \n17  1623418767882567680  @cpaul_ffc It’s like these 11 have n...   \n18  1623418319163510786  @RonFilipkowski Victim again ?!?!? N...   \n19  1623415846906241026  Should it be \"naked\" or \"nekkid?\"\\n\\...   \n\n                                 clean_text  \n1   Get my art printed on awesome produc...  \n2                          Trudeau? anyone?  \n3   Nuh uh. told me everything was fine....  \n4                     😂 she gave the 🤣 Af 😂  \n5   He’s afraid of me because of my eye ...  \n6   How old do I have to be before I hav...  \n7      Very funny alternative printed gifts  \n8   Hmm ... Can't attach pics, now, too?...  \n9   Looks like this toerag has really an...  \n10  I had a good laugh today responding ...  \n11  I wonder how many crypto commercials...  \n12  SOME_ENTITY So it's still on the tab...  \n13                     Real Cats Eat Hotdog  \n14         Some need to be informed this is  \n15  SOME_ENTITY It's so nice to have so ...  \n16  Funny Sassy Kitchen Towel - Embroide...  \n17  SOME_ENTITY_ffc It’s like these 11 h...  \n18  SOME_ENTITY Victim again ?!?!? Noooo...  \n19        Should it be \"naked\" or \"nekkid?\"  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>clean_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1623470696125923329</td>\n      <td>Get my art printed on awesome produc...</td>\n      <td>Get my art printed on awesome produc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1623467236982947842</td>\n      <td>Trudeau? anyone? #TuckerCarlson #Unh...</td>\n      <td>Trudeau? anyone?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1623465163792711681</td>\n      <td>Nuh uh. #joebiden told me everything...</td>\n      <td>Nuh uh. told me everything was fine....</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1623465117869395968</td>\n      <td>😂 she gave #Biden the #ChinaBalloon ...</td>\n      <td>😂 she gave the 🤣 Af 😂</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1623464838990295040</td>\n      <td>He’s afraid of me because of my eye ...</td>\n      <td>He’s afraid of me because of my eye ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1623463889194344449</td>\n      <td>How old do I have to be before I hav...</td>\n      <td>How old do I have to be before I hav...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1623463133170868224</td>\n      <td>https://t.co/DOHN1RsqCg… Very funny ...</td>\n      <td>Very funny alternative printed gifts</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1623461020382494720</td>\n      <td>Hmm ...  Can't attach pics, now, too...</td>\n      <td>Hmm ... Can't attach pics, now, too?...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1623460933900066816</td>\n      <td>Looks like this toerag has really an...</td>\n      <td>Looks like this toerag has really an...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1623434796432248833</td>\n      <td>I had a good laugh today responding ...</td>\n      <td>I had a good laugh today responding ...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1623433543203405827</td>\n      <td>I wonder how many crypto commercials...</td>\n      <td>I wonder how many crypto commercials...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1623430337786445824</td>\n      <td>@RepDonaldsPress So it's still on th...</td>\n      <td>SOME_ENTITY So it's still on the tab...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1623430319348264960</td>\n      <td>Real Cats Eat Hotdog\\nhttps://t.co/Z...</td>\n      <td>Real Cats Eat Hotdog</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1623429168745570309</td>\n      <td>Some need to be informed this is\\n#s...</td>\n      <td>Some need to be informed this is</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1623423936812748800</td>\n      <td>@DocumentingBTC \\n\\nIt's so nice to ...</td>\n      <td>SOME_ENTITY It's so nice to have so ...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1623420440717799427</td>\n      <td>Funny Sassy Kitchen Towel - Embroide...</td>\n      <td>Funny Sassy Kitchen Towel - Embroide...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1623418767882567680</td>\n      <td>@cpaul_ffc It’s like these 11 have n...</td>\n      <td>SOME_ENTITY_ffc It’s like these 11 h...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1623418319163510786</td>\n      <td>@RonFilipkowski Victim again ?!?!? N...</td>\n      <td>SOME_ENTITY Victim again ?!?!? Noooo...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1623415846906241026</td>\n      <td>Should it be \"naked\" or \"nekkid?\"\\n\\...</td>\n      <td>Should it be \"naked\" or \"nekkid?\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Puicking a sample for testing\nrng = np.random.default_rng(SEED)\nsample = rng.integers(low = 0, high = len(raw_data)-1,size =100)\n\ntest = raw_data.loc[sample,'text']\ntest","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:06.761316Z","iopub.execute_input":"2023-02-14T18:02:06.761647Z","iopub.status.idle":"2023-02-14T18:02:06.775160Z","shell.execute_reply.started":"2023-02-14T18:02:06.761619Z","shell.execute_reply":"2023-02-14T18:02:06.773815Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"635     @OV_Matter @samuelr28254292 @sinenom...\n85      @TheUfoJoe @SilvaRecord But this has...\n964     @SouthlandPost @FightHaven Oh, how l...\n177     Excited to share this item from my #...\n394     Make yourself the centre of attentio...\n714     At the book store\\n\\n#comics #stripc...\n533     Find your #scifi #fantasy #freebooks...\n1192    When regulators imposed a historic l...\n955     @ChanelRion @OANN Wait. \\nIs this on...\n148     Go out in style, with a #Logo from B...\n938     @Acyn How much education does Watter...\n205     @RonFilipkowski 🤔 I noticed Jim didn...\n325     Love is blind…. But…. #fun #Sarcasm ...\n880     @claudiastellner You don't lack cour...\n1183    @Reuters \\n\\n\"Glad to see that the g...\n198     @AlboMP About time. My contract expl...\n605     @EricPoppen1 Oh great, another day w...\n1179    You can't be 'ON' every day https://...\n145     #Cartoon of the Day\\n\\nClick the lin...\n302     I'm sure he couldn't wait for the re...\n728     @maggiesummer2 @RoseHowe1 @SenatorTi...\n110     @JasonSCampbell The world was much b...\n1171    My God! How absolutely shocking! #Sa...\n457              @Dsobie2 #sarcasm obviously 🙃🤣\n28      @creativitySUCKS @AJDelgado13 Yes......\n1024    I found secret leaked footage of #Te...\n585     What a mature response to being kick...\n993     Phew! Thank goodness Ja is still on ...\n1123    Forget the fold, or the flip...intro...\n154     Damn it, I don't have a gift for the...\n308     Lion roars are the loudest of any ot...\n186     Glad to see Kevin is interested in c...\n142     Excited to share the latest addition...\n702     @Pittounicos Yeah, I know. #Sarcasm ...\n319     @VOCMNEWS Get ready for the huge Bil...\n1089    In the Pacific Northwest, Bertrand s...\n1146    Thursday's #EsteeLauder sellers must...\n88          Nothing like a 5am start.  #sarcasm\n575     Article summary: https://t.co/KuJUje...\n841     Hey you! Yes you! Have you heard of ...\n218     @RepBentz Thanks for sharing your su...\n136     @jyotiTpandey05 Next order will be -...\n429     Meeting up with friends, why not imp...\n589     Stop using products/services which a...\n461     Goodnight Twitterverse, where tomorr...\n1190    You are gonna wake up too many peopl...\n583     @pich_bhootni @LimaGolf_ Harishankar...\n828     lard back - - #DadJokes #pun #humor ...\n766     @RpsAgainstTrump And Biden having th...\n406     @RepJackKimble Oh world, WATCH OUT, ...\n278     With all the recent addon/plugin dra...\n1165    @GOP Like Hunter Biden’s laptop! Tha...\n1140    \"Gaming &amp; Esports are so easy, a...\n891     Oh my gawd! I’m dumbfounded! 🙄\\n\\n#s...\n931     @TheFactFindr I have a simple soluti...\n858     I don’t see how that could turn prob...\n1074    BREAKUPS AND #SARCASM https://t.co/p...\n791     @Reuters Who would have thought Marc...\n407     @spittinchiclets Trouba leaves his f...\n1076    @Santos4Congress ChatGPT opinion on ...\n113     @_Mellayne @BigBroAfrica Dream on.Di...\n1137    My advice…. #fun #Sarcasm https://t....\n1004    I’m old enough to remember when PM B...\n251     @DebMac2022 @PeelIWG @nejsnave We do...\n194     @SenKatyG Yes, please. My contract e...\n93      @Emmonspired ChatGPT must have reali...\n985     Yes, #PossumKingdomLake is a real pl...\n103     @SimonPB I wonder who is paying for ...\n340     Wow, another day of sitting at home ...\n776     @Crussian17 I’m afraid to go long no...\n966     @FoxNews But it's not a natural gas ...\n919     The subject former journalist come W...\n1012    OK, that's enough interaction for to...\n854     @SarcasticRofl It's not anyone's rel...\n12      @RepDonaldsPress So it's still on th...\n387     @RepDonaldsPress It's NOT all bad.\\n...\n927     Move a bit from your position &amp; ...\n970     Does Science Fiction have energy sol...\n162     Cats: because nothing says 'I'm sorr...\n1143    THAT’S UNFORTUNATE https://t.co/Uwyx...\n681     Reasonable price if you ask me.     ...\n750     Balloon - Spiritually means a desire...\n1160    @ReduxxMag The best thing to ever ha...\n194     @SenKatyG Yes, please. My contract e...\n274     #Logo's make an excellent gift, why ...\n1118    I dn need a shoulder to cry on, \\n\\n...\n562     When texting that you’re dyeing you’...\n483     @TheHerd hey @BabyNewEARS think you ...\n480     UPDATE:  I was just told that this w...\n381     Figma vs Adobe XD? It's like choosin...\n641     I know jet pilots must play darts du...\n539     In a recent Public survey, Buckshot ...\n135     Sanjay Raut first used the word H\\n\\...\n238     “Titans Elevate Tim Kelly to Offensi...\n620     @ctrl_alt_rees Hey! Lemme point out ...\n130     @scorfano Maybe you should smile mor...\n603     @JoJoFromJerz @DonaldJTrumpJr We do ...\n845     Visit our #etsy shop: #tears of my #...\n644     @BranhamHolly @piyushmittal @trustin...\n42      @BaluchBenjimen Enna Jan Man Gusha k...\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"temp = clean_text(test,return_type = 'token')\ntemp","metadata":{"execution":{"iopub.status.busy":"2023-02-14T18:02:06.778337Z","iopub.execute_input":"2023-02-14T18:02:06.778746Z","iopub.status.idle":"2023-02-14T18:02:06.996624Z","shell.execute_reply.started":"2023-02-14T18:02:06.778687Z","shell.execute_reply":"2023-02-14T18:02:06.995076Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/27321743.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'token'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: clean_text() got an unexpected keyword argument 'return_type'"],"ename":"TypeError","evalue":"clean_text() got an unexpected keyword argument 'return_type'","output_type":"error"}]},{"cell_type":"markdown","source":"# Notes::\n\n1. Maybe number of hashtags as feature. ","metadata":{}}]}